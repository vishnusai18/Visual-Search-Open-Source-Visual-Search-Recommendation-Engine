{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70b8f9f9-b081-4e53-8254-4c4a2d986e9b",
   "metadata": {},
   "source": [
    "### This NoteBook Information\n",
    "- This Notebook contains the code & steps involved in generating embeddings from array & saving them in npy format for further usuaga in the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05556139-70e5-4dbc-8d0a-e35150fa4b51",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c353934f-637c-49de-9a77-d7b9316b03d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch # torch is used to deal with nueral network models using gpu\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel # this library is used to retrive the CLIP model and processor from hugging face platform\n",
    "from tqdm import tqdm # this library and function helps to see the progress of the task\n",
    "import requests # this library is used to check the protocols and proximity issues and connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9582a9-2634-406e-87e0-be6406350172",
   "metadata": {},
   "source": [
    "#### GPU Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "315cf64b-7b74-44ea-847d-269b2c31eb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__) # mandatory check before fetching the model\n",
    "print(\"CUDA available:\", torch.cuda.is_available()) # this step check whether the system has cuda enable gpu or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa8ceedd-0b5f-4d80-a3e0-cffee07f75ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0)) # this code give the exact nvidia gpu series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48beb726-ff51-449e-becf-a5c011a2c5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # selecting the gpu if there is cuda enables gpu on the system\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a93a44f-3f8d-424b-a0b6-0a9a7fda5025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc9dd77-52c6-4c77-88a7-58486f3a22f6",
   "metadata": {},
   "source": [
    "#### Fixed Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "688dbfcc-1270-4020-a6da-8661cf10bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGES_PATH = \"./Data/Images_Train/\"  # images path\n",
    "EMBEDDING_OUTPUT_DIR = \"embeddings\" # embeddings folder\n",
    "MODEL_NAME = \"openai/clip-vit-base-patch32\"  # CLIP Model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0356256-88a9-4975-b3b9-b4e54a9e5870",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(EMBEDDING_OUTPUT_DIR,exist_ok=True) # creating the embeddings folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a32a3586-556f-4cc9-96f1-9e6f29a00d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(requests.get(\"https://huggingface.co\").status_code) # checking the connection with hugging face "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1cb7e4-c9d8-4c57-b4fd-60b3a4894892",
   "metadata": {},
   "source": [
    "#### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b349df0-e990-448a-a4b3-5025a2183928",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(MODEL_NAME, use_safetensors=True).to(DEVICE) # loading model\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_NAME,use_fast=True) # loading its processor -> decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a503f86-7fe3-4b6a-a2c1-9597a28e6e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Generating CLIP embeddings on cuda for 60000 images...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_files = sorted([\n",
    "    f for f in os.listdir(TRAIN_IMAGES_PATH)\n",
    "    if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "])\n",
    "\n",
    "embeddings = []\n",
    "filenames = []\n",
    "\n",
    "print(f\"\\n🚀 Generating CLIP embeddings on {DEVICE} for {len(image_files)} images...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd90d9d-2859-4d9c-8209-7b82b5cbad96",
   "metadata": {},
   "source": [
    "#### Embeddings Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7dd5026-be0e-4a3f-810c-544091b8094c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 60000/60000 [1:12:34<00:00, 13.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Saved 60000 embeddings to 'embeddings/'\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0, len(image_files), BATCH_SIZE)):\n",
    "    batch_files = image_files[i:i + BATCH_SIZE]  # to avoid the gpu usuage limit we are making it in batches\n",
    "    batch_images = [] \n",
    "\n",
    "    for fname in batch_files:\n",
    "        try:\n",
    "            img_path = os.path.join(TRAIN_IMAGES_PATH, fname)\n",
    "            image = Image.open(img_path)\n",
    "            batch_images.append(image)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Skipping {fname}: {e}\")\n",
    "            continue\n",
    "\n",
    "    inputs = processor(images=batch_images, return_tensors=\"pt\").to(DEVICE) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "        image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "    for j, fname in enumerate(batch_files):\n",
    "        embeddings.append(image_features[j].cpu().numpy()) # storing the embeddings and file names\n",
    "        filenames.append(fname)\n",
    "\n",
    "# ====== SAVE ======\n",
    "np.save(os.path.join(EMBEDDING_OUTPUT_DIR, \"image_embeddings.npy\"), np.array(embeddings))\n",
    "np.save(os.path.join(EMBEDDING_OUTPUT_DIR, \"image_filenames.npy\"), np.array(filenames))\n",
    "\n",
    "print(f\"\\n✅ Saved {len(embeddings)} embeddings to '{EMBEDDING_OUTPUT_DIR}/'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
